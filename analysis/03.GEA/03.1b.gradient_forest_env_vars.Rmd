---
title: "03.1b.gradient_forest_env_vars"
author: "Erica Robertson"
date: "2025-10-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir="~/Desktop/BANS/BANS_adaptive_units/analysis/03.GEA")
```

Adapted from Holden Fox's gradient forest tutorial!!

#INTRODUCTION

The R package gradientForest utilizes a flexible non-parametric functions to quantify multi-species compositional turnover along environmental gradients. We can extend this to determine which variants are associated with specific environmental variables. So rather than thinking of the large-scale species turnover in space, we can map fine-scale allele-frequency changes along environmental gradients. 

Below is code for quantifying and visualizing intraspecific gene-environment variation across space. This code focuses on analyzing and plotting the allelic variation as estimated from gradient forest for a single species, the Bank Swallow.

Although you can use gradient forest as your main GEA method, there are some potential limitations of it. One of those is that it cannot explicitly account for population structure. So, a good approach for a more modern GEA is to use the GF to determine the best environmental predictors for allelic variation, and then use those environmental variables in an RDA. Redundancy Analysis allows for the inclusion population structure and thus is less biased in it's identification of associated SNPs.

Because we are going to be using WGS data, we'll want to run all of this in an R environment on the cluster. You can also subsample the data and run it on your local computer to perfect the code, although I've found that installing gradientForest on my local mac was a bit challenging.

We're gonna need two files for this run:
(1) allele frequencies per population, and
(2) a file(s) with env variables.

For this, we're going to be using per population allele frequencies. GF is a machine learning approach that fits a lon-linear relationship between genetic variation and environmental variables. It treats each population as a single data point, which averages out individual-level noise. RDA is a multivariate constrained ordination methods (basically a fancy linear regression) that detects linear relationships between genetic variation and environmental variables. It benefits from hjaving lots of independent observations to estimate patterns, so each individual provides one observation in the genotype space. More individuals increases statistical power, so we will run the RDA on individual level allele frequencies.

# CALCULATING PER POPULATION ALLELE FREQUENCIES
## Grouping Populations
When calculating allele frequencies per population, we want to define populations as a collection of 4 or more samples within 1 square latitude, longitude of each other. For gradientForest and RDA, we won't consider populations with a sample size < 4. We also need a vcf that doesn't have any missing data. So out options are to filter out all the data, or impute missing data. The first one is more stringent and will remove alot of SNPs. The second maintains SNPs but you are created artificial data so some people don't like to do it. If you've got time, I'd do both and see if it changes anything!

Gradient forest should install pretty easy on windows machines. Initially I had trouble installing it on an apple silicon macbook. You can install gradientForest on an apple-silicon mac by explicitly forcing the use of gcc-14 from homebrew instead of the default gcc (apple clang) with this command:

P.S. You should install gfortran (dependency for gradientForest) with homebrew first if using this route.

```{bash}
#get homebrew
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
#get gcc
brew install gcc
```

```{r}
install.packages("gradientForest", repos="http://R-Forge.R-project.org")
#, configure.vars = "CC=/opt/homebrew/bin/gcc-15 CXX=/opt/homebrew/bin/g++-15 FC=/opt/homebrew/bin/gfortran-15")
```

load dependencies
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(sf)
library(rnaturalearth)
library(geosphere)
library(plotly)
library(mapview)
library(dplyr)
```

So I've already put together the environmental data for each individual, that was done within the 01b.spatial_data.Rmd script.
```{r, message=FALSE, warning=FALSE}
meta <- read.delim("../../data/spatial_files/BANS.initial.worldclim.txt")
```

First I'm just going to map them to visualize where the points are laid out. This can give me a sense of what should be grouped in the next chunks of code.
```{r}
coords <- meta %>% select(Long, Lat)
coords_proj <- st_as_sf(meta, coords = c("Long", "Lat"), crs="+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")
mapview(coords_proj)
```

So let's see how we can group these individuals into populations.
```{r, message=FALSE, warning=FALSE}
# calculate distance matrix using Haversine distance
dist_matrix <- distm(as.matrix(coords), fun = distHaversine)

# clustering
hclust_groups <- hclust(as.dist(dist_matrix), method = "complete") # complete method here avoids chaining, ensuring all samples in a cluster are within the specified distance of each other. If you want some flexibility you could try "average"

# cut tree at 111.111 km to assign spatial clusters
meta$GroupID <- cutree(hclust_groups, h = 111111)

# create population labels and filter for minimum cluster size
meta.sub <- meta %>%
  mutate(Pop = paste0("Cluster_", GroupID)) %>%
  group_by(Pop) %>%
  filter(n() > 3) %>%  # keep clusters with ≥4 individuals
  ungroup()
```

Now that we have defined our populations, let's plot them and make sure they are what we intended.
```{r}
# import base map
library(rnaturalearth)
library(raster)
library(terra)
map <- ne_states(country = c("United States of America", "Canada", "Mexico"), 
                 returnclass = 'sf')

# import shrike range from ebird
BANS_range <- rast("../../data/spatial_files/banswa_abundance_seasonal_breeding_mean_2023.tif") %>% project(crs(map))

BANS_range.crop <- crop(BANS_range, map)
BANS_range.crop.xy <- as.data.frame(BANS_range.crop, xy=T)

# create colors for clusters
n_clusters <- length(unique(meta.sub$Pop))
cluster_colors <- rainbow(n_clusters)
names(cluster_colors) <- unique(meta.sub$Pop)

meta.b <- meta.sub %>%
  group_by(Pop) %>%
  mutate(cluster_size = n()) %>%
  ungroup()

# plot it
p <- ggplot() +
  geom_sf(data = map, fill = "white", color = "black", size = 0.1) +
  geom_raster(data = BANS_range.crop.xy, aes(x=x, y=y), fill = "grey", color = NA, alpha = 0.5) +
  geom_point(data = meta.b, aes(x = Long, 
                              y = Lat, 
                              color = Pop,
                              text = paste0(
                                "BGP_ID: ", BGP_ID, "<br>",
                                "Cluster: ", Pop, "<br>",
                                "Cluster Size: ", cluster_size, "<br>")), 
             size = 1.5, position = position_jitter(width = 0.1, height = 0.1)) +
  scale_colour_manual(values = cluster_colors, name = "Spatial Clusters", guide = "none") +
  coord_sf(xlim = c(-140, -65), ylim = c(15, 50)) + 
  theme_minimal() +
  labs(title = "All clusters with ≥4 individuals",
       x = "Longitude",
       y = "Latitude")

ggsave("test_range_plot.png")
#ggplotly(p, tooltip = "text")
```

Looks good. Now that we've defined our populations, let's create a file to filter with. Make the FID column (the first column) the same as in your .fam file. It's probably worth checking what that looks like now. It's likely either 0 or equal to IID. My FID == IID, so I'll populate both columns with BGP_ID here.
```{r}
pops <- data.frame(FID = meta.b$BGP_ID, IID = meta.b$BGP_ID, Group = meta.b$Pop) %>% 
  write.table(file = "BANS_sample_list_pop.tsv", sep= "\t", quote = FALSE, row.names = FALSE, col.names = FALSE)

#pops <- data.frame(FID = meta$BGP_ID) %>% write.table(file = "sample_list_269.txt", sep= "\t", quote = FALSE, row.names = FALSE, col.names = FALSE)
```

Let's pop over to the cluster where our filtered & imputed vcf file is for the allele frequency calculations in plink. You'll want to throw the pop.txt file you created up there too.
```{bash}
rsync -avzP /Users/ericarobertson/Desktop/BANS_adaptive_units/analysis/03.GEA/BANS_sample_list_pop.tsv ericacnr@colostate.edu@login.rc.colorado.edu:/scratch/alpine/ericacnr@colostate.edu/BANS/03.GEA/
```

## LD Pruning
I've imputed the vcf already, following a different script, but here I will include how I pruned for ld.

```{bash}
conda activate GWAS2
vcf="/scratch/alpine/ericacnr@colostate.edu/BANS/02.imputation/BANS.all.ds6x.pass-maf-0.05.bialSNP.filtered.ind03miss.0.8miss.imputed4.1.vcf.gz"
plink --vcf $vcf --aec --recode --out BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1

#manually adding SNP names to the .map file, chromosome.SNP#
cat BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.map | awk -F"\t" '{split($1, a, "."); print $1"\t"a[1]"."$4"\t"$3"\t"$4}' >  BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.rename.map

plink --ped BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.ped \
--map BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.rename.map \
--make-bed --aec \
--out BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1

cd ../03.GEA/

bfile="/scratch/alpine/ericacnr@colostate.edu/BANS/02.imputation/BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1"
plink --bfile $bfile --aec \
--set-missing-var-ids @:# \
--indep-pairwise 25 10 0.5 --out BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.ld25-10-0.5.vcf.gz
# Pruning complete.  153866 of 1738137 variants removed. -->
# Marker lists written to
# BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.ld25-10-0.5.vcf.gz.prune.in
# and
# BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.ld25-10-0.5.vcf.gz.prune.out
```

So for our plink command, we did the following:
--vcf - specified the location of our VCF file.
--update-ids recode.txt - this changed the FID and IID to what we wants instead of duplicated IIDs
--allow-extra-chr - allow additional chromosomes beyond the human chromosome set. This is necessary as otherwise plink expects chromosomes 1-22 and the human X chromosome.
--set-missing-var-ids - also necessary to set a variant ID for our SNPs. Human and model organisms often have annotated SNP names and so plink will look for these. We do not have them so instead we set ours to default to chromosome:position which can be achieved in plink by setting the option @:# - see here for more info.
--indep-pairwise - finally we are actually on the command that performs our linkage pruning! The first argument, 25 denotes we have set a window of 25 Kb. The second argument, 10 is our window step size - meaning we move 10 bp each time we calculate linkage. Finally, we set an r2 threshold - i.e. the threshold of linkage we are willing to tolerate. Here we prune any variables that show an r2 of greater than 0.5.
--out Produce the prefix for the output data.

Now that we've identified what SNPs are under linkage, we need to remove them. I'm doing an additional step here to make sure we narrow it down to just the individuals we're interested in.
```{bash}
plink --bfile $bfile --aec --allow-no-sex \
--extract BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.ld25-10-0.5.vcf.gz.prune.in \
--make-bed --pca --out BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.ld25-10-0.5
```

--extract - this just lets plink know we want to extract only these positions from our VCF - in other words, the analysis will only be conducted on these.
--make-bed - this is necessary to write out some additional files for another type of population structure analysis - a model based approach with admixture.
--pca - fairly self explanatory, this tells plink to calculate a principal components analysis.
ls
PCA output:
.eigenval - the eigenvalues from our analysis
.eigenvec- the eigenvectors from our analysis
plink binary output:
.bed - the cichlids bed file - this is a binary file necessary for admixture analysis. It is essentially the genotypes of the pruned dataset recoded as 1s and 0s.
.bim - a map file (i.e. information file) of the variants contained in the bed file.
.fam - a map file for the individuals contained in the bed file.

## Calculating per Population Allele Frequency
You will want to take the filtered, imputed, ld-pruned vcf you used for pop str and further filter individuals. Calculate per population allele frequency with plink.
```{bash}
vcf="/scratch/alpine/ericacnr@colostate.edu/BANS/02.imputation/BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.vcf.gz"
plink --bfile BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.ld25-10-0.5 --freq --within BANS_sample_list_pop.tsv --aec --out BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.ld25-10-0.5
```

## Subsample SNPs
This file is pretty big, so to test the data we'll subset a sample of 50k snps. 

Let's get an interactive node on alpine to do this. You'll want to read in the plink file, subset, select allele frequency(MAF), SNP ID (SNP) and population (CLST), then transpose so that the allele frequency for each population is a row, and the subset of snps are the columns.

```{bash}
sinteractive --partition=amilan --time=02:00:00 --ntasks=4 --qos=normal
mamba activate R
R
```

Let's use a loop to create 5 different set. We're going to sed seeds so it's reproducible, but the seed needs to change so that it's not randomly sampling the same set again and again.
```{r}
library(data.table)
library(tidyverse)
# SNP frequency data
df <- fread("BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.ld25-10-0.5.frq.strat", header = TRUE)

for (val in 1:5) {
  set.seed(40 + val)  # unique seed for each replicate (42, 43, 44, ...)
  sampled_snps <- df %>%
    distinct(SNP) %>%
    sample_n(50000)
  
  subset50k <- df %>%
    filter(SNP %in% sampled_snps$SNP) %>%
    select(SNP, CLST, MAF)
  
  # pivot to wide format (pops as rows, SNPs as columns)
  subset50k_wide <- subset50k %>%
    pivot_wider(names_from = SNP, values_from = MAF) %>%
    select(-CLST)  # remove CLST column after pivot
  
  out_file <- paste0("subsample_data/BANS.pop.frq.strat.50k.", val, ".txt")
  write.table(as.data.frame(subset50k_wide),
              file = out_file, row.names = FALSE, quote = FALSE, sep = "\t")
  
  message("Finished subset ", val, " → saved to ", out_file)
}
```

Let's move these onto my computer so I can work with them...

```{bash}
rsync -avzP ericacnr@colostate.edu@login.rc.colorado.edu:/scratch/alpine/ericacnr@colostate.edu/BANS/03.GEA/subsample_data /Users/ericarobertson/Desktop/BANS_adaptive_units/analysis/03.GEA 
```

# RETRIEVE ENVIRONMENTAL VARIABLES

Now we're loading in the environmental data that we got earlier.

So this is getting together the climate data for each population and filtering to pops (clusters) that have more than 4 samples. In my case this doesn't remove anything so it's fine.
```{r}
library(tidyverse)
pops <- read.table(file = "data/BANS_sample_list_pop.tsv", sep="\t", header = FALSE) %>% rename(BGP_ID = V1, Pop = V3) %>% dplyr::select(BGP_ID, Pop)

clim <- read_delim("data/BANS.initial.worldclim.landscape_edit.txt", delim = "\t") %>% 
  left_join(pops, by = "BGP_ID") %>%
  dplyr::select(starts_with("bio"), sand, clay, Pop) %>%
  group_by(Pop) %>% 
  summarise(
    n = n(),
    across(starts_with("bio"), mean, .names = "{.col}"),
    biosand = mean(sand, na.rm = TRUE),
    bioclay = mean(clay, na.rm = TRUE)
  ) %>% 
  filter(n > 4)
```
The populations file you create earlier in this markdown, and provide to plink to calculate allele frequencies within are almost certainly in the same order as the plink allele frequencies output. But just to be safe, I extract the order the populations are listed from the plink population allele frequency file, so that we can list the environmental predictors in the same order. Here is the command.
```{bash}
awk 'NR>1 { if(!seen[$3]++) print $3 }' BANS.ds6x.maf.0.05.SNP.above4x.maxmiss.8.imputed4.1.ld25-10-0.5.frq.strat > BANS-frq-pop-order.txt
```
```{bash}
rsync -avzP ericacnr@colostate.edu@login.rc.colorado.edu:/scratch/alpine/ericacnr@colostate.edu/BANS/03.GEA/BANS-frq-pop-order.txt /Users/ericarobertson/Desktop/BANS_adaptive_units/analysis/03.GEA
```

```{r}
## reorder so that env and alle frequencies per pop are reported in the same order
poporder <- read.table(file = "BANS-frq-pop-order.txt", header = F) %>% rename(Pop = V1)
clim_ordered <- poporder %>% left_join(clim) %>% dplyr::select(-n)

Ebans <- clim_ordered %>% dplyr::select(-Pop)

write.table(Ebans, file = "data/Bans_env.txt", row.names=F, quote=F, sep="\t")
```

# RUNNING GRADIENT FOREST
Two parts here. The first is to generate 5 GF models (one for each subset of SNPs we made before). For each model, we'll evaluate how well it's doing and get the environmental variables out. Then, part 2 is to compare across the 5 models we made to check for consistency.

## Generating and testing the models
So this is going to be making multiple GF models on different subsets of 50k SNPs, and the comparing everything to a bunch of null models with random environmental samples to see if the performance is okay.

This is taken an adapted from Caitlin Miller's code: https://github.com/mcaitlinv/bans-breeding/blob/main/05_au/scripts/gradFor_multiRun.R

So, this is a big chunk of code. I'm going to summarize what it's doing:
*For each SNP subset (1-5)*
1. builds a GF model for that subset
2. plots predictor importance for that model (importance plot)
3. checks for multicollinearity in environmental predictors (runs correlation matrix)
4. computes model performance metrics (R2, so the strength of GxE relationship)
5. provides randomization tests to tests the GF model quality (is the fit greater than random expectation?)
```{r}
library(gradientForest)
library(extendedForest)
for (val in 1:5) {
  name<-paste("BANS.pop.frq.strat.50k.",val,".txt", sep="")
  Gbans<- read_delim(paste0("subsample_data/", name))
  
  Ebans <- read_delim("Bans_env.txt", col_names=T)
  
  name<-paste("bans.16ClimGroup.minInd0.7.majmin4.scaf.noNA.50Krandom", val,".sort.220627", sep="")
  
  ##Specify predictors which are the environmentals vars
  preds <- colnames(Ebans)
  specs <- colnames(Ebans)
  
  nSites <- as.numeric(dim(Gbans)[1])
  nSpecs <- as.numeric(dim(Gbans)[2])
  
  # # set depth of conditional permutation
  lev <- floor(log2(nSites*0.368/2))
  
  ##Making the gradient forest model with env as pred and response as genomic sites
  ##This takes awhile and spits out thousands of lines of errors
  ##Ignore the errors
  bansforest=gradientForest(cbind(Ebans,Gbans), 
                            predictor.vars=preds, 
                            response.vars=specs, 
                            ntree=500, transform = NULL, 
                            compact=T,nbin=101, 
                            maxLevel=lev,trace=T)
  
  saveRDS(bansforest, file = paste("results/GF_outputs/bansforest", name,".rds", sep=""))
  bansforest<-readRDS(paste("results/GF_outputs/bansforest", name,".rds", sep=""))
  #Check predictor importance
  allpredictors=names(importance(bansforest))
  ##Save as pdf 
  pdf(paste("results/GF_outputs/", name, ".importance.pdf", sep=""))
  plot(bansforest,plot.type="O")
  dev.off()
  
  ##Check for correlation in variables
  ##Aiming to take the top 10 variables and pare down until max number of variables with corr <0.7
  cor<-cor(Ebans,method="pearson")
  cor %>% write.table(paste("results/GF_outputs/", name, ".corr", sep=""),quote=F,sep="\t",row.names=T)
  
  ##Start randomization to check how accurate model is
  realtotal=bansforest$species.pos.rsq
  realaverage=sum(bansforest$result)/realtotal
  
  #Create randomization values for 100 different runs of random values
  #Paste into table
  for (i in 1:100) {
    ##Sample randomly the rows of the environment
    predsR=Ebans[sample(nrow(Ebans)),]
  
    ##Build your model with the randomly sampled rows of the environment
    ##These will be mismatched to the rows of genomic data, thereby randomizing your predictors
    bansforestR=gradientForest(cbind(predsR,Gbans), predictor.vars=colnames(predsR), response.vars=colnames(Gbans), ntree=100, transform = NULL, compact=F, maxLevel=1,trace=F)
    ##Find the random total r-sq and average r-sq
    randtotal=bansforestR$species.pos.rsq
    randaverage=sum(bansforestR$result)/randtotal
  
    ##Write random values into tables
    write.table(randtotal,file=paste("results/GF_outputs/",name,"bslmm.nobin.randtotal",sep=""),row.names=FALSE,col.names=FALSE,append=T)
    write.table(randaverage,file=paste("results/GF_outputs/",name,"bslmm.nobin.randaverage",sep=""),row.names=FALSE,col.names=FALSE,append=T)
  }
  
  ##Read in the 500 random averages and r-squareds
  randa=read.table(paste("results/GF_outputs/",name,"bslmm.nobin.randtotal",sep=""))
  randt=read.table(paste("results/GF_outputs/",name,"bslmm.nobin.randaverage",sep=""))
 
  quantile(randa$V1,c(0.995))
  
  hist(randa$V1,main="Average R-squared of Random Gradient Forests",xlab="Average R-squared of SNPs", xlim =c(0.09, 0.18))

  abline(v=realaverage,col="red")
  ##Make a graph showing the distribution of randoms
  pdf(paste("results/GF_outputs/", name, ".randomization.hist.par.pdf", sep=""))
  par(mfrow=c(2,1))
  hist(randa$V1,main="Average R-squared of Random Gradient Forests",xlab="Average R-squared of SNPs", xlim =c(0.09, 0.18))
  abline(v=realaverage,col="red") #replace with your actual number from realaverage
  dev.off()
 }
```

## Comparing across models
Now we have stats for each model, let's compare across models. We hope that it's pretty consistent! ChatGPT is going to make this code for me.

```{r}
model_dir <- "results/GF_outputs/"

# initialize results storage
gf_summary <- data.frame(
  subset = integer(),
  mean_rsq = numeric(),
  sd_rsq = numeric(),
  total_rsq = numeric(),
  top_predictor = character(),
  top_importance = numeric(),
  stringsAsFactors = FALSE
)

# loop through the 5 GF models you created
for (val in 1:5) {
  
  name <- paste0("bans.16ClimGroup.minInd0.7.majmin4.scaf.noNA.50Krandom", val, ".sort.220627")
  model_file <- paste0(model_dir, "bansforest", name, ".rds")
  
  if (!file.exists(model_file)) {
    message("Skipping missing file: ", model_file)
    next
  }
  
  # load model
  gf <- readRDS(model_file)
  
  # compute key metrics
  rsq_vals <- gf$species.pos.rsq
  mean_rsq <- mean(rsq_vals, na.rm = TRUE)
  sd_rsq   <- sd(rsq_vals, na.rm = TRUE)
  total_rsq <- sum(rsq_vals, na.rm = TRUE)
  
  # predictor importance
  imp <- importance(gf)
  imp_sorted <- sort(imp, decreasing = TRUE)
  top_pred <- names(imp_sorted)[1]
  top_imp  <- imp_sorted[1]
  
  # store summary
  gf_summary <- rbind(
    gf_summary,
    data.frame(
      subset = val,
      mean_rsq = mean_rsq,
      sd_rsq = sd_rsq,
      total_rsq = total_rsq,
      top_predictor = top_pred,
      top_importance = top_imp,
      stringsAsFactors = FALSE
    )
  )
  
  # save importance table for each model
  write.table(
    data.frame(Predictor = names(imp_sorted), Importance = imp_sorted),
    file = paste0(model_dir, "importance_rank_", val, ".txt"),
    sep = "\t", quote = FALSE, row.names = FALSE
  )
}

# write summary table
write.table(gf_summary,
            file = paste0(model_dir, "GF_subset_summary.txt"),
            sep = "\t", quote = FALSE, row.names = FALSE)

# plot quick comparison
ggplot(gf_summary, aes(x = factor(subset), y = mean_rsq)) +
  geom_point(size = 3) +
  geom_line(aes(group = 1)) +
  theme_bw() +
  labs(x = "Subset number", y = "Mean R² across SNPs",
       title = "Gradient Forest model performance across SNP subsets")

print(gf_summary)
```

